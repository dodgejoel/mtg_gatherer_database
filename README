This bundle of code is designed to scrape Wizards of the Coast's Magic: the
Gathering website at 'http://gatherer.wizards.com' and build a SQLite database
called mtg_gatherer.db with all of the card information contained in it. First
run initialize_database.py to create the database with the appropriate tables.
Next run gather_card_data.py to extract all of the data. This takes a while
because each cards page has to be opened up one at a time. Finally run
input_to_database.py to input the gathered data into the database. 

If gather_card_data.py is interrupted and then called again, it will pick up
where it left off. i.e. the card data already retrieved will not be fetched
again. 

This is an ongoing project. The next step is to bundle this into a single
script that, when called once, will execute the three parts of this program
without further user interaction. After this, I will implement a GUI for
interacting with the database. In essence I would like to recreate locally all
of the functionality of the original website.

Non built-in modules required are: bs4, sqlite3.

Bugs/Problems: 

1. Fuse/Split/Flip cards are all supported now as well as werewolves and other
Innistrad/Dark Ascension flip cards. Only the name, mana cost and card text is
currently show for both halves with each halves data seperated by a '//'.
Gatherer's treatment of these cards is not uniform so there are some funny
things.  Currently all werewolves have two multiverse_ids for example whereas
split cards only have one.This will need to be dealt with in a more heavy
handed way to avoid having duplicate werewolves in the resulting database. In
addition, I would like to seperate out the types/ and subtypes of each half of
these cards as many of them change.  The parsing of the type subtype data is a
little different than he rest of the data so I am putting this off for now.   

2. There is a difficult to reproduce bug involving URL request timeouts and
some of the threads in the gather_card_data script not finishing their tasks
and not terminating correctly. Perhaps has something to do with the speed of
the internet connection? Not a serious problem as the program can always be run
a second time to clean up the lost jobs.  I still have not understood this
well.

