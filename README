This bundle of code is designed to scrape Wizards of the Coast's Magic: the
Gathering website at 'http://gatherer.wizards.com' and build a SQLite database
called mtg_gatherer.db with all of the card information contained in it. First
run initialize_database.py to create the database with the appropriate tables.
Next run gather_card_data.py to extract all of the data. This takes a while
because each cards page has to be opened up one at a time. Finally run
input_to_database.py to input the gathered data into the database. 

If gather_card_data.py is interrupted and then called again, it will pick up
where it left off. i.e. the card data already retrieved will not be fetched
again. 

This is an ongoing project. I would like to bundle this into a single script
that, when called once, will execute the three parts of this program without
further user interaction. After this, I would like to implement a GUI for
interacting with the database. In essence I would like to recreate locally all
of the functionality of the original website.

Non built-in modules required are: bs4, sqlite3.

Bugs/Problems: 

1. We only pick up one half of each fuse and split card. Parsing the HTML is a
little bit different in this case. Can be fixed easily.

2. Difficult to reproduce bug involving URL request timeouts and some of the
threads in the gather_card_data script not finishing their tasks and not
terminating correctly. Perhaps has something to do with the speed of the
internet connection? Not a serious problem as the program can always be run a
second time to clean up the lost jobs.

